#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —Å–∏—Å—Ç–µ–º—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ –µ–¥–∏–Ω—ã–π –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
"""

import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

from config.settings import settings
from core.logger import get_logger, scraping_metrics
from scrapper import YandexMapsScraper, BusinessData
from exporters import UnifiedExporter
from models import Enterprise


class ExtractionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.successful_extractions: List[BusinessData] = []
        self.failed_extractions: List[Dict] = []
        self.total_urls: int = 0
        self.processing_time: float = 0.0
        self.export_paths: Dict[str, str] = {}
        self.errors: List[str] = []
        
    @property
    def success_rate(self) -> float:
        """–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏–π"""
        return (len(self.successful_extractions) / self.total_urls * 100) if self.total_urls > 0 else 0.0
    
    def add_successful(self, data: BusinessData, url: str):
        """–î–æ–±–∞–≤–∏—Ç—å —É—Å–ø–µ—à–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ"""
        self.successful_extractions.append(data)
        
    def add_failed(self, url: str, error: str):
        """–î–æ–±–∞–≤–∏—Ç—å –Ω–µ—É–¥–∞—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ"""
        self.failed_extractions.append({
            'url': url,
            'error': error,
            'timestamp': datetime.now().isoformat()
        })
        
    def get_summary(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        return {
            'total_urls': self.total_urls,
            'successful': len(self.successful_extractions),
            'failed': len(self.failed_extractions),
            'success_rate': round(self.success_rate, 2),
            'processing_time': round(self.processing_time, 2),
            'avg_time_per_url': round(self.processing_time / self.total_urls, 2) if self.total_urls > 0 else 0,
            'export_formats': list(self.export_paths.keys()),
            'timestamp': datetime.now().isoformat()
        }


class EnterpriseDataExtractor:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —Å–∏—Å—Ç–µ–º—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö —Å –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π API
    –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è—Ö.
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
        
        Args:
            config: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        self.config = config or {}
        self.logger = get_logger(__name__)
        self.exporter = UnifiedExporter()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏
        self.session_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_processing_time': 0.0,
            'session_start': datetime.now()
        }
        
        self.logger.info("EnterpriseDataExtractor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def extract_single(
        self, 
        url: str,
        export_formats: List[str] = None,
        output_dir: str = None,
        include_services: bool = True,
        include_reviews: bool = True,
        max_reviews: int = 50
    ) -> Dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è –Ω–∞ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç–∞—Ö
            export_formats: –§–æ—Ä–º–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∞ ['json', 'csv', 'database']
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
            include_services: –ò–∑–≤–ª–µ–∫–∞—Ç—å —É—Å–ª—É–≥–∏
            include_reviews: –ò–∑–≤–ª–µ–∫–∞—Ç—å –æ—Ç–∑—ã–≤—ã
            max_reviews: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å –ø—É—Ç—è–º–∏ –∫ —Ñ–∞–π–ª–∞–º
        """
        start_time = time.time()
        self.session_stats['total_requests'] += 1
        
        if export_formats is None:
            export_formats = ['json']
            
        if output_dir:
            self.exporter = UnifiedExporter(output_dir)
        
        self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: {url}")
        
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
            if not self._validate_url(url):
                raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL: {url}")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            with YandexMapsScraper(self.config) as scraper:
                business_data = scraper.scrape_business(url, max_reviews)
                
                if not business_data:
                    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è")
            
            # –≠–∫—Å–ø–æ—Ä—Ç –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            export_results = {}
            for format_type in export_formats:
                try:
                    file_path = self.exporter.export_by_format(business_data, format_type)
                    export_results[format_type] = file_path
                    self.logger.info(f"–î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {format_type}: {file_path}")
                except Exception as e:
                    export_results[f"{format_type}_error"] = str(e)
                    self.logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ {format_type}: {e}")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.session_stats['successful_requests'] += 1
            processing_time = time.time() - start_time
            self.session_stats['total_processing_time'] += processing_time
            
            result = {
                'success': True,
                'data': business_data.model_dump() if business_data else None,
                'export_paths': export_results,
                'processing_time': round(processing_time, 2),
                'url': url,
                'timestamp': datetime.now().isoformat(),
                'data_quality': self._assess_data_quality(business_data) if business_data else {}
            }
            
            self.logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ –∑–∞ {processing_time:.2f}s")
            return result
            
        except Exception as e:
            self.session_stats['failed_requests'] += 1
            processing_time = time.time() - start_time
            
            error_result = {
                'success': False,
                'error': str(e),
                'url': url,
                'processing_time': round(processing_time, 2),
                'timestamp': datetime.now().isoformat()
            }
            
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ {url}: {e}")
            return error_result
    
    def extract_batch(
        self,
        urls: List[str],
        export_formats: List[str] = None,
        output_dir: str = None,
        delay_between: float = None,
        max_workers: int = 1,
        continue_on_error: bool = True
    ) -> ExtractionResult:
        """
        –ú–∞—Å—Å–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–ø–∏—Å–∫–∞ URL
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            export_formats: –§–æ—Ä–º–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∞
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            delay_between: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
            max_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ (–¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
            continue_on_error: –ü—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
            
        Returns:
            ExtractionResult: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        start_time = time.time()
        result = ExtractionResult()
        result.total_urls = len(urls)
        
        if export_formats is None:
            export_formats = ['json']
            
        if delay_between is None:
            delay_between = (settings.MIN_DELAY + settings.MAX_DELAY) / 2
        
        self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –º–∞—Å—Å–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ: {len(urls)} URL")
        
        for i, url in enumerate(urls, 1):
            self.logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL {i}/{len(urls)}: {url}")
            
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è
                extraction_result = self.extract_single(
                    url=url,
                    export_formats=export_formats,
                    output_dir=output_dir
                )
                
                if extraction_result['success']:
                    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç BusinessData –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    business_data = BusinessData(**extraction_result['data'])
                    result.add_successful(business_data, url)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞
                    for format_type, path in extraction_result.get('export_paths', {}).items():
                        if format_type not in result.export_paths:
                            result.export_paths[format_type] = []
                        if isinstance(result.export_paths[format_type], list):
                            result.export_paths[format_type].append(path)
                        else:
                            result.export_paths[format_type] = [result.export_paths[format_type], path]
                else:
                    result.add_failed(url, extraction_result.get('error', 'Unknown error'))
                    
            except Exception as e:
                result.add_failed(url, str(e))
                if not continue_on_error:
                    self.logger.error(f"–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏: {e}")
                    break
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ)
            if i < len(urls):
                self.logger.debug(f"–ü–∞—É–∑–∞ {delay_between}s –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º")
                time.sleep(delay_between)
        
        result.processing_time = time.time() - start_time
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç –¥–ª—è —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if result.successful_extractions and 'json' in export_formats:
            try:
                summary_path = self.exporter.json_exporter.export_multiple(
                    result.successful_extractions,
                    format_type='object_collection'
                )
                result.export_paths['batch_summary'] = summary_path
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        summary = result.get_summary()
        self.logger.info(f"–ú–∞—Å—Å–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {summary}")
        scraping_metrics.log_summary()
        
        return result
    
    def extract_from_file(
        self,
        file_path: Union[str, Path],
        **kwargs
    ) -> ExtractionResult:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º URL
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å URL (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)
            **kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è extract_batch
            
        Returns:
            ExtractionResult: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        
        # –ß–∏—Ç–∞–µ–º URL –∏–∑ —Ñ–∞–π–ª–∞
        with open(file_path, 'r', encoding='utf-8') as f:
            urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        
        if not urls:
            raise ValueError(f"–í —Ñ–∞–π–ª–µ {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö URL")
        
        self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(urls)} URL –∏–∑ —Ñ–∞–π–ª–∞ {file_path}")
        
        return self.extract_batch(urls, **kwargs)
    
    def validate_urls(self, urls: List[str]) -> Dict[str, List[str]]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ URL
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            Dict: –°–ª–æ–≤–∞—Ä—å —Å –≤–∞–ª–∏–¥–Ω—ã–º–∏ –∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º–∏ URL
        """
        valid_urls = []
        invalid_urls = []
        
        for url in urls:
            if self._validate_url(url):
                valid_urls.append(url)
            else:
                invalid_urls.append(url)
        
        return {
            'valid': valid_urls,
            'invalid': invalid_urls,
            'valid_count': len(valid_urls),
            'invalid_count': len(invalid_urls),
            'total_count': len(urls)
        }
    
    def get_session_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏"""
        current_time = datetime.now()
        session_duration = (current_time - self.session_stats['session_start']).total_seconds()
        
        stats = {
            **self.session_stats,
            'session_duration_seconds': round(session_duration, 2),
            'success_rate': round(
                (self.session_stats['successful_requests'] / max(self.session_stats['total_requests'], 1)) * 100, 2
            ),
            'avg_processing_time': round(
                self.session_stats['total_processing_time'] / max(self.session_stats['successful_requests'], 1), 2
            ),
            'requests_per_minute': round(
                self.session_stats['total_requests'] / max(session_duration / 60, 1), 2
            )
        }
        
        return stats
    
    def _validate_url(self, url: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è URL –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–µ–º—É
            if parsed.scheme not in ['http', 'https']:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
            domain_valid = any(
                domain in parsed.netloc 
                for domain in settings.YANDEX_MAPS_DOMAINS
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å
            path_valid = '/maps/' in parsed.path
            
            return domain_valid and path_valid
            
        except Exception:
            return False
    
    def _assess_data_quality(self, business_data: BusinessData) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not business_data:
            return {'score': 0.0, 'completeness': 0.0}
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –ø–æ–ª—è
        total_fields = 10  # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
        filled_fields = 0
        
        if business_data.name:
            filled_fields += 1
        if business_data.category:
            filled_fields += 1
        if business_data.address:
            filled_fields += 1
        if business_data.phone:
            filled_fields += 1
        if business_data.website:
            filled_fields += 1
        if business_data.rating is not None:
            filled_fields += 1
        if business_data.reviews_count is not None:
            filled_fields += 1
        if business_data.services:
            filled_fields += 1
        if business_data.social_networks and (
            business_data.social_networks.telegram or 
            business_data.social_networks.whatsapp or 
            business_data.social_networks.vk
        ):
            filled_fields += 1
        if business_data.working_hours and (
            business_data.working_hours.current_status or 
            business_data.working_hours.schedule
        ):
            filled_fields += 1
        
        completeness = filled_fields / total_fields
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_metrics = {
            'completeness': round(completeness, 2),
            'has_contact_info': bool(business_data.phone or business_data.website),
            'has_services': len(business_data.services) > 0,
            'has_reviews': len(business_data.reviews) > 0,
            'has_rating': business_data.rating is not None,
            'has_social_networks': bool(
                business_data.social_networks and any([
                    business_data.social_networks.telegram,
                    business_data.social_networks.whatsapp,
                    business_data.social_networks.vk
                ])
            ),
            'services_count': len(business_data.services),
            'reviews_count': len(business_data.reviews),
            'data_richness_score': round(
                (len(business_data.services) * 0.2 + 
                 len(business_data.reviews) * 0.1 + 
                 (1 if business_data.rating else 0) * 0.3 +
                 completeness * 0.4), 2
            )
        }
        
        return quality_metrics


# Convenience —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def extract_single_enterprise(url: str, **kwargs) -> Dict:
    """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è"""
    extractor = EnterpriseDataExtractor()
    return extractor.extract_single(url, **kwargs)


def extract_multiple_enterprises(urls: List[str], **kwargs) -> ExtractionResult:
    """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–π"""
    extractor = EnterpriseDataExtractor()
    return extractor.extract_batch(urls, **kwargs)


def extract_from_file(file_path: str, **kwargs) -> ExtractionResult:
    """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞"""
    extractor = EnterpriseDataExtractor()
    return extractor.extract_from_file(file_path, **kwargs)


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    extractor = EnterpriseDataExtractor()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π URL (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π)
    test_url = "https://yandex.com.ge/maps/-/CHXU6Fmb"
    
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ EnterpriseDataExtractor")
    print("=" * 50)
    
    try:
        result = extractor.extract_single(
            url=test_url,
            export_formats=['json', 'csv'],
            include_services=True,
            include_reviews=True
        )
        
        if result['success']:
            print("‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìä –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result['processing_time']}s")
            print(f"üìÅ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
            for format_type, path in result['export_paths'].items():
                print(f"   ‚Ä¢ {format_type}: {path}")
            
            # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
            data = result.get('data', {})
            print(f"\nüìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–∏:")
            print(f"   ‚Ä¢ –ù–∞–∑–≤–∞–Ω–∏–µ: {data.get('name', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}")
            print(f"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {data.get('category', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}")
            print(f"   ‚Ä¢ –†–µ–π—Ç–∏–Ω–≥: {data.get('rating', '–ù–µ —É–∫–∞–∑–∞–Ω–æ')}")
            print(f"   ‚Ä¢ –£—Å–ª—É–≥ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(data.get('services', []))}")
            print(f"   ‚Ä¢ –û—Ç–∑—ã–≤–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(data.get('reviews', []))}")
            
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:", result['error'])
            
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ—Å—Å–∏–∏
    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏:")
    stats = extractor.get_session_statistics()
    for key, value in stats.items():
        if key != 'session_start':
            print(f"   ‚Ä¢ {key}: {value}")