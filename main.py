#!/usr/bin/env python3
"""
–û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∏—Å—Ç–µ–º—ã —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç
"""
import argparse
import json
import os
import sys
import time
from pathlib import Path
from typing import List, Optional

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ Python path
sys.path.insert(0, str(Path(__file__).parent))

from config.settings import settings
from core.logger import get_logger, scraping_metrics
from scrapper import BusinessData, YandexMapsScraper


def setup_directories():
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
    directories = [Path(settings.OUTPUT_PATH), Path("logs"), Path("data")]

    for directory in directories:
        directory.mkdir(exist_ok=True)


def save_results(
    data: BusinessData, output_format: str = "json", output_path: str = None
) -> str:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞

    Args:
        data: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        output_format: –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ (json, csv)
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

    Returns:
        str: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    if output_path is None:
        output_path = settings.OUTPUT_PATH

    output_dir = Path(output_path)
    output_dir.mkdir(exist_ok=True)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è –∏ –≤—Ä–µ–º–µ–Ω–∏
    safe_name = "".join(
        c for c in data.name if c.isalnum() or c in (" ", "-", "_")
    ).rstrip()
    safe_name = safe_name.replace(" ", "_")[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

    timestamp = data.scraping_date.strftime("%Y%m%d_%H%M%S")
    filename = f"{safe_name}_{timestamp}"

    if output_format.lower() == "json":
        filepath = output_dir / f"{filename}.json"

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        json_data = data.model_dump(mode="json")

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)

    elif output_format.lower() == "csv":
        import pandas as pd

        filepath = output_dir / f"{filename}.csv"

        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        basic_data = {
            "name": data.name,
            "category": data.category,
            "rating": data.rating,
            "reviews_count": data.reviews_count,
            "address": data.address,
            "phone": data.phone,
            "website": data.website,
            "services_count": len(data.services),
            "reviews_extracted": len(data.reviews),
            "scraping_date": data.scraping_date.isoformat(),
        }

        df = pd.DataFrame([basic_data])
        df.to_csv(filepath, index=False, encoding="utf-8")

    return str(filepath)


def scrape_single_url(url: str, output_format: str = "json") -> Optional[str]:
    """
    –°–∫—Ä–∞–ø–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ URL

    Args:
        url: URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        output_format: –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞

    Returns:
        str: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–ª–∏ None
    """
    logger = get_logger(__name__)

    try:
        with YandexMapsScraper() as scraper:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥: {url}")

            result = scraper.scrape_business(url)

            if result:
                filepath = save_results(result, output_format)
                logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filepath}")
                return filepath
            else:
                logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ")
                return None

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
        return None


def scrape_multiple_urls(
    urls: List[str], output_format: str = "json", delay_between: float = None
) -> List[str]:
    """
    –°–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö URL

    Args:
        urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        output_format: –§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞
        delay_between: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    logger = get_logger(__name__)
    results = []

    if delay_between is None:
        delay_between = (settings.MIN_DELAY + settings.MAX_DELAY) / 2

    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥ {len(urls)} URL")

    for i, url in enumerate(urls, 1):
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL {i}/{len(urls)}: {url}")

        try:
            result_path = scrape_single_url(url, output_format)
            if result_path:
                results.append(result_path)

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ)
            if i < len(urls):
                logger.info(f"–ü–∞—É–∑–∞ {delay_between}s –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º")
                time.sleep(delay_between)

        except KeyboardInterrupt:
            logger.warning("–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
            continue

    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£—Å–ø–µ—à–Ω–æ: {len(results)}/{len(urls)}")
    scraping_metrics.log_summary()

    return results


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(
        description="–°–∏—Å—Ç–µ–º–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –°–∫—Ä–∞–ø–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è
  python main.py --url "https://yandex.com.ge/maps/-/CHXU6Fmb"
  
  # –°–∫—Ä–∞–ø–∏–Ω–≥ –∏–∑ —Ñ–∞–π–ª–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º URL
  python main.py --file urls.txt --format csv
  
  # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
  python main.py --file urls.txt --delay 10
        """,
    )

    parser.add_argument("--url", type=str, help="URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è")
    parser.add_argument(
        "--file", type=str, help="–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º URL (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)"
    )
    parser.add_argument(
        "--format",
        choices=["json", "csv"],
        default="json",
        help="–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: json)",
    )
    parser.add_argument(
        "--output", type=str, help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
    )
    parser.add_argument(
        "--delay", type=float, help="–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö"
    )
    parser.add_argument(
        "--debug", action="store_true", help="–í–∫–ª—é—á–∏—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π —Ä–µ–∂–∏–º"
    )

    args = parser.parse_args()

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–ª–∞–¥–∫–∏
    if args.debug:
        os.environ["DEBUG"] = "True"
        os.environ["LOG_LEVEL"] = "DEBUG"

    # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    setup_directories()

    logger = get_logger(__name__)
    logger.info("–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç")

    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –≤—ã–≤–æ–¥–∞ –µ—Å–ª–∏ –∑–∞–¥–∞–Ω
    if args.output:
        settings.OUTPUT_PATH = args.output

    try:
        if args.url:
            # –°–∫—Ä–∞–ø–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ URL
            result = scrape_single_url(args.url, args.format)
            if result:
                print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {result}")
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ")
                sys.exit(1)

        elif args.file:
            # –°–∫—Ä–∞–ø–∏–Ω–≥ –∏–∑ —Ñ–∞–π–ª–∞
            urls_file = Path(args.file)
            if not urls_file.exists():
                print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.file}")
                sys.exit(1)

            # –ß–∏—Ç–∞–µ–º URL –∏–∑ —Ñ–∞–π–ª–∞
            with open(urls_file, "r", encoding="utf-8") as f:
                urls = [line.strip() for line in f if line.strip()]

            if not urls:
                print(f"‚ùå –í —Ñ–∞–π–ª–µ {args.file} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∞–ª–∏–¥–Ω—ã—Ö URL")
                sys.exit(1)

            print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(urls)} URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

            results = scrape_multiple_urls(urls, args.format, args.delay)

            print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            print(f"üìä –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(results)}/{len(urls)}")

            if results:
                print("\nüìÅ –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
                for result_path in results:
                    print(f"  ‚Ä¢ {result_path}")
        else:
            print("‚ùå –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å --url –∏–ª–∏ --file")
            parser.print_help()
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
