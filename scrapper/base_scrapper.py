"""
–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö —Å –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç
"""

import json
import re
import time
from datetime import datetime
from typing import Any, Dict, List, Optional, Union
from urllib.parse import urlparse

from bs4 import BeautifulSoup
from pydantic import BaseModel, Field, ValidationError
from selenium import webdriver
from selenium.common.exceptions import TimeoutException, WebDriverException
from tenacity import retry, stop_after_attempt, wait_exponential

from config.chrome_config import ChromeConfig, user_agent_rotator
from config.settings import settings
from core.logger import get_logger, log_scraping_stats, scraping_metrics

from .navigation import NavigationError, YandexMapsNavigator
from .selectors import selectors


class ServiceData(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö —É—Å–ª—É–≥–∏"""

    name: str
    price: Optional[str] = None
    price_from: Optional[str] = None
    price_to: Optional[str] = None
    description: Optional[str] = None
    duration: Optional[str] = None


class SocialNetworks(BaseModel):
    """–ú–æ–¥–µ–ª—å —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π"""

    telegram: Optional[str] = None
    whatsapp: Optional[str] = None
    vk: Optional[str] = None


class WorkingHours(BaseModel):
    """–ú–æ–¥–µ–ª—å —Ä–∞–±–æ—á–∏—Ö —á–∞—Å–æ–≤"""

    current_status: Optional[str] = None
    schedule: Optional[Dict[str, str]] = None
    notes: Optional[str] = None


class ReviewData(BaseModel):
    """–ú–æ–¥–µ–ª—å –æ—Ç–∑—ã–≤–∞"""

    author: str
    rating: Optional[int] = None
    date: Optional[str] = None
    text: Optional[str] = None
    response: Optional[str] = None


class BusinessData(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è"""

    name: str
    category: Optional[str] = None
    services: List[ServiceData] = Field(default_factory=list)
    website: Optional[str] = None
    address: Optional[str] = None
    social_networks: SocialNetworks = Field(default_factory=SocialNetworks)
    phone: Optional[str] = None
    working_hours: WorkingHours = Field(default_factory=WorkingHours)
    rating: Optional[float] = None
    ratings_count: Optional[int] = None  # –î–æ–±–∞–≤–ª—è–µ–º - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫ (102)
    reviews_count: Optional[int] = None  # –û—Å—Ç–∞–≤–ª—è–µ–º - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ (89)
    reviews: List[ReviewData] = Field(default_factory=list)
    scraping_date: datetime = Field(default_factory=datetime.now)
    metadata: Dict[str, Any] = Field(default_factory=dict)


class YandexMapsScraper:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç"""

    def __init__(self, config: Optional[Dict] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞

        Args:
            config: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        """
        self.config = config or {}
        self.logger = get_logger(__name__)
        self.driver: Optional[webdriver.Chrome] = None
        self.navigator: Optional[YandexMapsNavigator] = None
        self.start_time: Optional[float] = None

    def __enter__(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        self.initialize_driver()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        self.cleanup()

    def initialize_driver(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è WebDriver"""
        try:
            self.logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Chrome WebDriver")

            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–π User-Agent
            user_agent = None
            if settings.ROTATE_USER_AGENT:
                user_agent = user_agent_rotator.get_best_user_agent()
                self.logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è User-Agent: {user_agent[:50]}...")

            # –°–æ–∑–¥–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä
            self.driver = ChromeConfig.create_driver(user_agent)
            self.navigator = YandexMapsNavigator(self.driver)

            self.logger.info("WebDriver —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ WebDriver: {e}")
            raise

    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.driver:
            try:
                self.driver.quit()
                self.logger.info("WebDriver –∑–∞–∫—Ä—ã—Ç")
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ WebDriver: {e}")

    def validate_url(self, url: str) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è URL –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç

        Args:
            url: URL –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏

        Returns:
            bool: True –µ—Å–ª–∏ URL –≤–∞–ª–∏–¥–µ–Ω
        """
        try:
            parsed = urlparse(url)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–µ–º—É
            if parsed.scheme not in ["http", "https"]:
                return False

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
            domain_valid = any(
                domain in parsed.netloc for domain in settings.YANDEX_MAPS_DOMAINS
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å
            path_valid = "/maps/" in parsed.path

            return domain_valid and path_valid

        except Exception:
            return False

    @retry(
        stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2, min=4, max=10)
    )
    def load_page(self, url: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Returns:
            bool: True –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ
        """
        if not self.validate_url(url):
            raise ValueError(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π URL: {url}")

        self.logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
        self.start_time = time.time()

        try:
            self.driver.get(url)

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not self.navigator.wait_for_page_load():
                raise TimeoutException("–°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –≤ –æ—Ç–≤–µ–¥–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∞–ø—á—É
            if self.navigator.check_for_captcha():
                raise NavigationError("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞")

            # –ò–º–∏—Ç–∏—Ä—É–µ–º –ø–æ–≤–µ–¥–µ–Ω–∏–µ —á–µ–ª–æ–≤–µ–∫–∞
            self.navigator.simulate_human_behavior()

            self.logger.info("–°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

            return True

        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            raise

    def navigate_to_services_tab(self) -> bool:
        """
        –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –≤–∫–ª–∞–¥–∫—É —É—Å–ª—É–≥

        Returns:
            bool: True –µ—Å–ª–∏ –ø–µ—Ä–µ—Ö–æ–¥ —É—Å–ø–µ—à–µ–Ω
        """
        return self.navigator.navigate_to_services_tab()

    def navigate_to_reviews_tab(self) -> bool:
        """
        –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –≤–∫–ª–∞–¥–∫—É –æ—Ç–∑—ã–≤–æ–≤

        Returns:
            bool: True –µ—Å–ª–∏ –ø–µ—Ä–µ—Ö–æ–¥ —É—Å–ø–µ—à–µ–Ω
        """
        return self.navigator.navigate_to_reviews_tab()

    def _extract_owner_response(self, review_element) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤–ª–∞–¥–µ–ª—å—Ü–∞ –Ω–∞ –æ—Ç–∑—ã–≤

        Args:
            review_element: –≠–ª–µ–º–µ–Ω—Ç –æ—Ç–∑—ã–≤–∞

        Returns:
            Optional[str]: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –≤–ª–∞–¥–µ–ª—å—Ü–∞ –∏–ª–∏ None
        """
        try:
            # –ò—â–µ–º –∫–Ω–æ–ø–∫—É "–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ—Ç–≤–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"
            response_button = self.navigator.find_element_with_fallback(
                selectors.REVIEWS["review_response_button"], review_element
            )

            if not response_button:
                return None

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–Ω–æ–ø–∫–∞ –¥–ª—è –ø–æ–∫–∞–∑–∞ –æ—Ç–≤–µ—Ç–∞
            button_text = response_button.text.strip().lower()
            if (
                "–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ—Ç–≤–µ—Ç" not in button_text
                and "–ø–æ–∫–∞–∑–∞—Ç—å –æ—Ç–≤–µ—Ç" not in button_text
            ):
                return None

            # –ö–ª–∏–∫–∞–µ–º –ø–æ –∫–Ω–æ–ø–∫–µ
            if self.navigator.safe_click(response_button):
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –æ—Ç–≤–µ—Ç–∞
                time.sleep(1)

                # –ò—â–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç–≤–µ—Ç–∞ –≤ —Ç–æ–º –∂–µ review_element –∏–ª–∏ —Ä—è–¥–æ–º —Å –Ω–∏–º
                # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Å–∞–º–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ –æ—Ç–∑—ã–≤–∞
                response_content = self.navigator.find_element_with_fallback(
                    selectors.REVIEWS["review_response_content"], review_element
                )

                # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —ç–ª–µ–º–µ–Ω—Ç–µ –æ—Ç–∑—ã–≤–∞, –∏—â–µ–º –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                if not response_content:
                    parent_element = review_element.find_element_by_xpath("./..")
                    response_content = self.navigator.find_element_with_fallback(
                        selectors.REVIEWS["review_response_content"], parent_element
                    )

                if response_content:
                    response_text = response_content.text.strip()
                    self.logger.debug(
                        f"–ù–∞–π–¥–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {response_text[:50]}..."
                    )
                    return response_text

            return None

        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {e}")
            return None

    def _parse_price(self, price_text: str) -> Dict[str, str]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞

        Args:
            price_text: –¢–µ–∫—Å—Ç —Å —Ü–µ–Ω–æ–π

        Returns:
            Dict: –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–∞—è —Ü–µ–Ω–∞
        """
        result = {}

        if not price_text:
            return result

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        clean_text = re.sub(r"[^\d\s\-‚Äì‚Äî–æ—Ç –¥–æ‚ÇΩ]", "", price_text)

        # –ò—â–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, "2000-3000" –∏–ª–∏ "–æ—Ç 2000 –¥–æ 3000")
        range_match = re.search(r"(\d+)\s*[-‚Äì‚Äî]\s*(\d+)", clean_text)
        if range_match:
            result["price_from"] = range_match.group(1)
            result["price_to"] = range_match.group(2)
            result["price"] = f"{result['price_from']}-{result['price_to']}"
            return result

        # –ò—â–µ–º "–æ—Ç X" –∏–ª–∏ "–¥–æ X"
        from_match = re.search(r"–æ—Ç\s*(\d+)", clean_text)
        to_match = re.search(r"–¥–æ\s*(\d+)", clean_text)

        if from_match:
            result["price_from"] = from_match.group(1)
            result["price"] = f"–æ—Ç {result['price_from']}"
        elif to_match:
            result["price_to"] = to_match.group(1)
            result["price"] = f"–¥–æ {result['price_to']}"
        else:
            # –ò—â–µ–º –ø—Ä–æ—Å—Ç–æ–µ —á–∏—Å–ª–æ
            simple_match = re.search(r"(\d+)", clean_text)
            if simple_match:
                result["price"] = simple_match.group(1)

        return result

    def scrape_business(
        self, url: str, max_reviews: int = 50
    ) -> Optional[BusinessData]:
        """
        –ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–∏

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è

        Returns:
            BusinessData: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None
        """
        if not self.driver:
            raise RuntimeError("WebDriver –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        self.logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫—Ä–∞–ø–∏–Ω–≥: {url}")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            if not self.load_page(url):
                return None

            # –ü–∞—É–∑–∞ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
            self.navigator.random_delay()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            basic_data = self.extract_basic_info()

            if not basic_data.get("name"):
                self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏—è")
                return None

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏
            social_data = self.extract_social_networks()

            # –î–∞–Ω–Ω—ã–µ —É—Å–ª—É–≥
            services_data = []
            if self.navigate_to_services_tab():
                self.navigator.random_delay()
                services_data = self.extract_services()

            # –î–∞–Ω–Ω—ã–µ –æ—Ç–∑—ã–≤–æ–≤
            reviews_data = []
            if self.navigate_to_reviews_tab():
                self.navigator.random_delay()
                reviews_data = self.extract_reviews(max_reviews=max_reviews)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
            actual_reviews_count = getattr(self, '_actual_reviews_count', None)

            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –¥–∞–Ω–Ω—ã—Ö
            business_data = BusinessData(
                name=basic_data.get("name", ""),
                category=basic_data.get("category"),
                services=[ServiceData(**service) for service in services_data],
                website=basic_data.get("website"),
                address=basic_data.get("address"),
                social_networks=SocialNetworks(**social_data),
                phone=basic_data.get("phone"),
                working_hours=WorkingHours(**basic_data.get("working_hours", {})),
                rating=basic_data.get("rating"),
                ratings_count=basic_data.get("ratings_count"),  # 102
                reviews_count=actual_reviews_count,  # 89
                reviews=[ReviewData(**review) for review in reviews_data],
                metadata={
                    "source_url": url,
                    "scraper_version": "1.0",
                    "processing_time_sec": (
                        round(time.time() - self.start_time, 2)
                        if self.start_time
                        else 0
                    ),
                },
            )

            # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            processing_time = time.time() - self.start_time if self.start_time else 0
            data_extracted = (
                len(services_data)
                + len(reviews_data)
                + len([k for k, v in basic_data.items() if v])
            )

            log_scraping_stats(url, True, processing_time, data_extracted)
            scraping_metrics.record_request(True, processing_time)

            self.logger.info(f"–°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞ {processing_time:.2f}s")
            return business_data

        except Exception as e:
            processing_time = time.time() - self.start_time if self.start_time else 0
            error_msg = str(e)

            log_scraping_stats(url, False, processing_time, 0, error_msg)
            scraping_metrics.record_request(False, processing_time, error_msg)

            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞: {e}")
            return None

    def extract_services(self) -> List[Dict[str, Any]]:
        """
        –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Å–ª—É–≥ —á–µ—Ä–µ–∑ BeautifulSoup –ø–æ—Å–ª–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ Selenium

        Returns:
            List: –°–ø–∏—Å–æ–∫ —É—Å–ª—É–≥
        """
        self.logger.info("–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —É—Å–ª—É–≥ (–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥)")

        services = []

        # 1. –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
        if not self.navigate_to_services_tab():
            self.logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ –≤–∫–ª–∞–¥–∫—É —É—Å–ª—É–≥")
            return services

        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        time.sleep(3)

        # 2. –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        self.logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        page_html = self.driver.page_source

        soup = BeautifulSoup(page_html, "html.parser")

        # 4. –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —É—Å–ª—É–≥
        service_elements = soup.find_all(
            "div", class_="business-full-items-grouped-view__item"
        )

        if not service_elements:
            self.logger.warning("–£—Å–ª—É–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ HTML")
            return services

        total_elements = len(service_elements)
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {total_elements} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —É—Å–ª—É–≥ –≤ HTML")

        # 5. Progress bar –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        progress_step = max(1, total_elements // 20)  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 5%

        for i, element in enumerate(service_elements, 1):
            try:
                # Progress bar
                if i % progress_step == 0 or i == total_elements:
                    percentage = (i / total_elements) * 100
                    filled = int(percentage // 5)
                    bar = "‚ñà" * filled + "‚ñë" * (20 - filled)
                    print(
                        f"\r‚ö° –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: [{bar}] {percentage:.0f}% ({i}/{total_elements}) | –ù–∞–π–¥–µ–Ω–æ: {len(services)}",
                        end="",
                        flush=True,
                    )

                service_data = {}

                # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ (grid —Ç–∏–ø)
                name_elem = element.find("div", class_="related-item-photo-view__title")
                # Fallback –¥–ª—è list —Ç–∏–ø–∞
                if not name_elem:
                    name_elem = element.find(
                        "div", class_="related-item-list-view__title"
                    )

                if not name_elem:
                    continue

                service_name = name_elem.get_text(strip=True)
                if not service_name:
                    continue

                service_data["name"] = service_name

                # –ò—â–µ–º —Ü–µ–Ω—É (grid —Ç–∏–ø)
                price_elem = element.find("span", class_="related-product-view__price")
                # Fallback –¥–ª—è list —Ç–∏–ø–∞
                if not price_elem:
                    price_elem = element.find(
                        "div", class_="related-item-list-view__price"
                    )

                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    service_data.update(self._parse_price(price_text))

                # –ò—â–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                desc_elem = element.find(
                    "div", class_="related-item-photo-view__description"
                )
                # Fallback –¥–ª—è list —Ç–∏–ø–∞
                if not desc_elem:
                    desc_elem = element.find(
                        "div", class_="related-item-list-view__subtitle"
                    )

                if desc_elem:
                    desc_text = desc_elem.get_text(strip=True)
                    if desc_text:
                        service_data["description"] = desc_text

                services.append(service_data)

            except Exception as e:
                self.logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É—Å–ª—É–≥–∏ #{i}: {e}")
                continue

        # –ó–∞–≤–µ—Ä—à–∞–µ–º progress bar
        print()
        self.logger.info(
            f"‚ö° –ë—ã—Å—Ç—Ä–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(services)} —É—Å–ª—É–≥ –∑–∞ {time.time() - time.time():.1f}s"
        )
        return services

    def extract_basic_info(self) -> Dict[str, Any]:
        """
        –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ BeautifulSoup

        Returns:
            Dict: –°–ª–æ–≤–∞—Ä—å —Å –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        """
        self.logger.info("–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")

        # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        page_html = self.driver.page_source

        # –ü–∞—Ä—Å–∏–º —á–µ—Ä–µ–∑ BeautifulSoup
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(page_html, "html.parser")

        data = {}

        # –ù–∞–∑–≤–∞–Ω–∏–µ
        name_elem = soup.find("a", class_="card-title-view__title-link")
        if name_elem:
            data["name"] = name_elem.get_text(strip=True)
            self.logger.debug(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {data['name']}")

        # –ö–∞—Ç–µ–≥–æ—Ä–∏—è
        category_elem = soup.find("a", class_="business-categories-view__category")
        if category_elem:
            data["category"] = category_elem.get_text(strip=True)
            self.logger.debug(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {data['category']}")

        # –†–µ–π—Ç–∏–Ω–≥
        rating_elem = soup.find(
            "span", class_="business-rating-badge-view__rating-text"
        )
        if rating_elem:
            rating_text = rating_elem.get_text(strip=True)
            rating_match = re.search(r"(\d+[\.,]\d+)", rating_text)
            if rating_match:
                data["rating"] = float(rating_match.group(1).replace(",", "."))
                self.logger.debug(f"–†–µ–π—Ç–∏–Ω–≥: {data['rating']}")

        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫ (–±—ã–ª–æ reviews_count)
        reviews_elem = soup.find("div", class_="business-header-rating-view__text")
        if reviews_elem:
            reviews_text = reviews_elem.get_text(strip=True)
            reviews_match = re.search(r"(\d+)", reviews_text)
            if reviews_match:
                data["ratings_count"] = int(reviews_match.group(1))  # –ò–∑–º–µ–Ω–µ–Ω–æ
                self.logger.debug(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫: {data['ratings_count']}")

        # –ê–¥—Ä–µ—Å
        address_elem = soup.find("div", class_="business-contacts-view__address-link")
        if address_elem:
            data["address"] = address_elem.get_text(strip=True)
            self.logger.debug(f"–ê–¥—Ä–µ—Å: {data['address']}")

        # –¢–µ–ª–µ—Ñ–æ–Ω
        phone_elem = soup.find("div", class_="card-phones-view__phone-number")
        if phone_elem:
            data["phone"] = phone_elem.get_text(strip=True)
            self.logger.debug(f"–¢–µ–ª–µ—Ñ–æ–Ω: {data['phone']}")

        # –°–∞–π—Ç
        website_elem = soup.find("a", class_="business-urls-view__link")
        if website_elem:
            href = website_elem.get("href")
            if href:
                data["website"] = href
                self.logger.debug(f"–°–∞–π—Ç: {data['website']}")

        # –ß–∞—Å—ã —Ä–∞–±–æ—Ç—ã
        working_elem = soup.find("div", class_="business-working-status-view")
        if working_elem:
            data["working_hours"] = {
                "current_status": working_elem.get_text(strip=True)
            }
            self.logger.debug(
                f"–°—Ç–∞—Ç—É—Å —Ä–∞–±–æ—Ç—ã: {data['working_hours']['current_status']}"
            )

        return data

    def extract_reviews(self, max_reviews: int = -1) -> List[Dict[str, Any]]:
        """
        –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ BeautifulSoup + Selenium –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
        """
        self.logger.info(f"–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–∑—ã–≤–æ–≤{'(–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)' if max_reviews == -1 else f'(–º–∞–∫—Å. {max_reviews})'}")
        
        reviews = []
        
        # 1. –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –≤–∫–ª–∞–¥–∫—É –æ—Ç–∑—ã–≤–æ–≤ —á–µ—Ä–µ–∑ Selenium
        if not self.navigate_to_reviews_tab():
            self.logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ –≤–∫–ª–∞–¥–∫—É –æ—Ç–∑—ã–≤–æ–≤")
            return reviews
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        time.sleep(3)
        
        # 2. –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        self.logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ—Ç–∑—ã–≤–∞–º–∏...")
        page_html = self.driver.page_source
        
        # 3. –ü–∞—Ä—Å–∏–º —á–µ—Ä–µ–∑ BeautifulSoup
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(page_html, 'html.parser')
        
        # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        reviews_header = soup.find('h2', class_='card-section-header__title')
        if reviews_header:
            header_text = reviews_header.get_text(strip=True)
            reviews_count_match = re.search(r'(\d+)\s*–æ—Ç–∑—ã–≤', header_text)
            if reviews_count_match:
                actual_reviews_count = int(reviews_count_match.group(1))
                self.logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {actual_reviews_count}")
                self._actual_reviews_count = actual_reviews_count
        
        # 5. –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –æ—Ç–∑—ã–≤—ã - –ë–û–õ–ï–ï –¢–û–ß–ù–´–ô –°–ï–õ–ï–ö–¢–û–†
        review_elements = soup.find_all('div', class_='business-review-view__info')
        
        if not review_elements:
            self.logger.warning("–û—Ç–∑—ã–≤—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ HTML")
            return reviews
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
        if max_reviews > 0:
            review_elements = review_elements[:max_reviews]
        
        total_reviews = len(review_elements)
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {total_reviews} –æ—Ç–∑—ã–≤–æ–≤ –≤ HTML")
        
        # 5. Progress bar –¥–ª—è –æ—Ç–∑—ã–≤–æ–≤
        progress_step = max(1, total_reviews // 20)
        
        for i, element in enumerate(review_elements, 1):
            try:
                # Progress bar
                if i % progress_step == 0 or i == total_reviews:
                    percentage = (i / total_reviews) * 100
                    filled = int(percentage // 5)
                    bar = "‚ñà" * filled + "‚ñë" * (20 - filled)
                    print(f"\rüí¨ –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∑—ã–≤–æ–≤: [{bar}] {percentage:.0f}% ({i}/{total_reviews}) | –ù–∞–π–¥–µ–Ω–æ: {len(reviews)}", end="", flush=True)
                
                review_data = {}
                
                # –ê–≤—Ç–æ—Ä
                author_elem = element.find('span', attrs={'itemprop': 'name'})
                if author_elem:
                    review_data["author"] = author_elem.get_text(strip=True)
                
                # –†–µ–π—Ç–∏–Ω–≥ –∏–∑ aria-label
                rating_elem = element.find('div', class_='business-rating-badge-view__stars')
                if rating_elem:
                    aria_label = rating_elem.get('aria-label', '')
                    rating_match = re.search(r"(?:–û—Ü–µ–Ω–∫–∞|Rating) (\d+) (?:–ò–∑|Out of)", aria_label, re.IGNORECASE)
                    if rating_match:
                        review_data["rating"] = int(rating_match.group(1))
                
                # –î–∞—Ç–∞ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
                date_elem = element.find('span', class_='business-review-view__date')
                if date_elem:
                    # –ò—â–µ–º span —Å —Ç–µ–∫—Å—Ç–æ–º –¥–∞—Ç—ã
                    date_span = date_elem.find('span')
                    if date_span:
                        review_data["date"] = date_span.get_text(strip=True)
                    # Fallback - –∏—â–µ–º meta —Å datePublished
                    elif date_elem.find('meta', attrs={'itemprop': 'datePublished'}):
                        content = date_elem.find('meta')['content']
                        try:
                            from datetime import datetime
                            dt = datetime.fromisoformat(content.replace('Z', '+00:00'))
                            review_data["date"] = dt.strftime('%d %B')
                        except:
                            review_data["date"] = content
                
                # –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
                text_elem = element.find('span', class_='spoiler-view__text-container') or \
                        element.find('div', class_='business-review-view__body')
                if text_elem:
                    review_data["text"] = text_elem.get_text(strip=True)
                
                # –û—Ç–≤–µ—Ç –≤–ª–∞–¥–µ–ª—å—Ü–∞ - –ì–ò–ë–†–ò–î–ù–´–ô –ü–û–î–•–û–î
                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ HTML, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º Selenium
                response_elem = element.find('div', class_='business-review-comment-content__bubble')
                if response_elem:
                    review_data["response"] = response_elem.get_text(strip=True)
                else:
                    # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ –≤–∏–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –∫–Ω–æ–ø–∫—É –∏ –∫–ª–∏–∫–Ω—É—Ç—å —á–µ—Ä–µ–∑ Selenium
                    response_button = element.find('div', class_='business-review-view__comment-expand')
                    if response_button:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium –¥–ª—è –∫–ª–∏–∫–∞ (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –Ω–∞–¥–µ–∂–Ω–µ–µ)
                        try:
                            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç —á–µ—Ä–µ–∑ Selenium
                            selenium_elements = self.navigator.find_element_with_fallback(
                                selectors.REVIEWS["review_items"]
                            )
                            if selenium_elements and isinstance(selenium_elements, list) and i <= len(selenium_elements):
                                selenium_element = selenium_elements[i-1]
                                response_text = self._extract_owner_response(selenium_element)
                                if response_text:
                                    review_data["response"] = response_text
                        except:
                            pass  # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —á–µ—Ä–µ–∑ Selenium, –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–º
                
                # –í –∫–æ–Ω—Ü–µ —Ü–∏–∫–ª–∞ for –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –æ—Ç–∑—ã–≤–∞:
                if review_data.get("author") and len(review_data.get("author", "")) > 1:
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –æ—Ç–∑—ã–≤ –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ö–æ—Ç—è –±—ã –∞–≤—Ç–æ—Ä–∞ –∏ —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ä–µ–π—Ç–∏–Ω–≥
                    if review_data.get("text") or review_data.get("rating"):
                        reviews.append(review_data)
                    
            except Exception as e:
                self.logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–∑—ã–≤–∞ #{i}: {e}")
                continue
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º progress bar
        print()
        self.logger.info(f"‚ö° –ë—ã—Å—Ç—Ä–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤")
        return reviews

        # –ó–∞–≤–µ—Ä—à–∞–µ–º progress bar
        print()
        self.logger.info(f"‚ö° –ë—ã—Å—Ç—Ä–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(reviews)} –æ—Ç–∑—ã–≤–æ–≤")
        return reviews

    def extract_social_networks(self) -> Dict[str, str]:
        """
        –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ —á–µ—Ä–µ–∑ BeautifulSoup

        Returns:
            Dict: –°–ª–æ–≤–∞—Ä—å —Å —Å–æ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏
        """
        self.logger.debug("–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π")

        # –ü–æ–ª—É—á–∞–µ–º HTML –µ—Å–ª–∏ –µ—â–µ –Ω–µ –ø–æ–ª—É—á–∏–ª–∏
        page_html = self.driver.page_source

        from bs4 import BeautifulSoup

        soup = BeautifulSoup(page_html, "html.parser")

        social_data = {}

        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        all_links = soup.find_all("a", href=True)

        for link in all_links:
            href = link.get("href", "")

            # Telegram
            if ("t.me" in href or "telegram" in href) and "telegram" not in social_data:
                social_data["telegram"] = href
                self.logger.debug(f"telegram: {href}")

            # WhatsApp
            elif (
                "wa.me" in href or "whatsapp" in href
            ) and "whatsapp" not in social_data:
                social_data["whatsapp"] = href
                self.logger.debug(f"whatsapp: {href}")

            # VK
            elif "vk.com" in href and "vk" not in social_data:
                social_data["vk"] = href
                self.logger.debug(f"vk: {href}")

        return social_data
